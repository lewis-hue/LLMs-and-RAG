<p align="center">
  <img src="https://img.shields.io/badge/LLMâ€“RAGâ€“Google%20Vertex%20AI-blue" alt="Tech Stack Badge" />
  <img src="https://img.shields.io/badge/Flaskâ€“Python-green" alt="Backend Badge" />
  <img src="https://img.shields.io/badge/HTMLâ€“CSSâ€“JS-yellow" alt="Frontend Badge" />
</p>

# <code>ğŸ¯ Qubiten Compliance Chatbot</code>

> **An intelligent, Retrieval-Augmented LLM assistant** powering compliance guidance for ISO 27001, SOC 2, HIPAA & GDPR.

---

## ğŸ“‘ Table of Contents

1. [Project Summary](#project-summary)  
2. [Architecture Diagram](#architecture-diagram)  
3. [LLM & RAG Components](#llm--rag-components)  
   - [LLM Model](#llm-model)  
   - [Retrieval-Augmented Generation](#retrieval-augmented-generation)  
4. [Data Preparation & Vertex AI Tuning](#data-preparation--vertex-ai-tuning)  
5. [Backend Implementation](#backend-implementation)  
6. [Frontend Implementation](#frontend-implementation)  
7. [Integration & Testing](#integration--testing)  
8. [Conclusion](#conclusion)

---

## ğŸ” Project Summary

- **Company Simulation:** Qubiten â€“ Compliance services provider  
- **Technologies:**  
  - **LLM & RAG:** Google Vertex AI LLM (`text-bison@001`)  
  - **Backend:** Python, Flask, Vertex AI REST API  
  - **Frontend:** HTML, CSS, JavaScript  
  - **Deployment:** Local & third-party site integration  
- **Key Features:**  
  1. Tuned KB dataset on Vertex AI  
  2. Document embeddings & similarity search  
  3. Interactive chat widget overlay  
  4. End-to-end flow: user â†’ retrieval â†’ LLM â†’ response  

**Skills & Keywords:**  
LLM Â· RAG Â· Google Vertex AI Â· Embeddings Â· Flask Â· Python Â· JavaScript Â· HTML Â· CSS Â· REST API Â· Chatbot Â· Architecture Â· Documentation

---

## ğŸ—ï¸ Architecture Diagram

```mermaid
flowchart TD
  A[ğŸ‘¤ User] -->|sends query| B[ğŸŒ Web UI]
  B -->|POST /predict| C[ğŸ Flask Backend]
  C -->|embed query| D{ğŸ” Vector Store}
  C -->|similarity search| D
  D -->|top-k docs| C
  C -->|construct prompt| E[ğŸ¤– Vertex AI LLM<br/>text-bison@001]
  E -->|generates answer| C
  C -->|return JSON| B
  B -->|render| A
Figure 1: High-level flow from user request to response.
ğŸ¤– LLM & RAG Components
â–¶ï¸ LLM Model
text
Copy
Edit
Model: text-bison@001 (Google Vertex AI)
Why text-bison@001?

High throughput & cost-efficient

Excellent instruction-following

ğŸ“š Retrieval-Augmented Generation
Document Ingestion

Compliance guides, whitepapers, policy docs

Embedding Generation

text
Copy
Edit
Vertex AI Embeddings API â†’ 768-dim vectors
Vector Store

Local FAISS / Pinecone index

Query & Retrieval

Embed user query â†’ retrieve top 5 nearest docs

Prompt Assembly

text
Copy
Edit
[SYSTEM]
You are Qubitenâ€™s compliance assistant...
[CONTEXT]
<doc1>â€¦<doc5>
[USER]
{user question}
LLM Call & Post-processing

Send prompt â†’ receive data.answer â†’ format response

ğŸ§° Data Preparation & Vertex AI Tuning
bash
Copy
Edit
# 1. Upload base model
gcloud ai models upload \
  --region=us-central1 \
  --display-name=qubiten-llm \
  --container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/text-bison@001

# 2. Create tuning job
gcloud ai tuning-jobs create \
  --model=projects/.../models/qubiten-llm \
  --training-dataset=projects/.../datasets/qa_pairs \
  --parameter-split=0.8 \
  --machine-type=n1-standard-4
Corpus: ISO 27001, SOC 2, HIPAA, GDPR specs

Steps: Cleaning â†’ Tokenization â†’ Q&A annotation â†’ GCS upload â†’ fine-tune

Outcome: Domain-specific accuracy boost

ğŸ”§ Backend Implementation
python
Copy
Edit
from flask import Flask, request, jsonify
from vertexai import MatchingEngine, TextGenerationModel

app = Flask(__name__)
index = MatchingEngine.Index("projects/.../locations/.../indexes/qubiten-index")
llm   = TextGenerationModel.from_pretrained("text-bison@001")

@app.route("/predict", methods=["POST"])
def predict():
    user_msg = request.json["message"]
    vec      = index.embed([user_msg])[0]
    docs     = index.search(vec, top_k=5)
    prompt   = assemble_prompt(docs, user_msg)
    resp     = llm.predict(prompt)
    return jsonify(answer=resp.text)
Endpoint: POST /predict

Workflow:

Embed query

Retrieve docs

Build prompt

Call LLM

Return JSON

ğŸ’» Frontend Implementation
html
Copy
Edit
<!-- index.html -->
<script src="nav-dynamic.js"></script>
js
Copy
Edit
// nav-dynamic.js (chat snippet)
const response = await fetch("http://127.0.0.1:5000/predict", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({ message: text })
});
const data = await response.json();
bubble(data.answer, "bot");
Files:

index.html â€“ main shell + chat widget

nav-dynamic.js â€“ overlays, chat integration

styles.css â€“ bespoke utility styles

UX: Smooth toggle, typing indicator, auto-scroll

âœ… Integration & Testing
Local Dev:

Flask on localhost:5000

Latency < 500 ms

3rd-Party Site:

Embedded via <script>

CORS configured

Results:

100% demo uptime

Excellent response relevance

ğŸ Conclusion
Qubiten Compliance Chatbot showcases:

Mastery of RAG and LLM fine-tuning

Clean, modular Flask + JS architecture

Seamless local & third-party integration

Production-ready, extensible to any regulated domain
